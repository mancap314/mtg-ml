{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Based Card Classification\n",
    "## Introduction\n",
    "The goal of this notebook is to classify the cards based on their *oracle_text* with deep learning models\n",
    "\n",
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "df_path = join(*['..', '..', 'data', 'cards-tags', 'tagged_cards.csv'])\n",
    "word2vec_path = join(*['../../data/word2vec.txt.gz'])\n",
    "\n",
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create numerical labels from the cards tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>oracle_text</th>\n",
       "      <th>oracleid</th>\n",
       "      <th>tag</th>\n",
       "      <th>type_line</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abomination of Gudul</td>\n",
       "      <td>Flying\\nWhenever Abomination of Gudul deals co...</td>\n",
       "      <td>3d98af5f-7a0b-4a5a-b3e4-f3c9d150c993</td>\n",
       "      <td>discard-outlet</td>\n",
       "      <td>Creature — Horror</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Academy Elite</td>\n",
       "      <td>Academy Elite enters the battlefield with X +1...</td>\n",
       "      <td>ba6c3c72-c014-45c6-a0b4-59eb9a65303e</td>\n",
       "      <td>discard-outlet</td>\n",
       "      <td>Creature — Human Wizard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Academy Raider</td>\n",
       "      <td>Intimidate (This creature can't be blocked exc...</td>\n",
       "      <td>75131d75-0703-44d0-b503-35190be8e66f</td>\n",
       "      <td>discard-outlet</td>\n",
       "      <td>Creature — Human Warrior</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akoum Flameseeker</td>\n",
       "      <td>Cohort — {T}, Tap an untapped Ally you control...</td>\n",
       "      <td>efae637f-3232-46f2-9839-f3386e2f447d</td>\n",
       "      <td>discard-outlet</td>\n",
       "      <td>Creature — Human Shaman Ally</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexi, Zephyr Mage</td>\n",
       "      <td>{X}{U}, {T}, Discard two cards: Return X targe...</td>\n",
       "      <td>3f60de36-ed63-4d08-a012-fc16e91da46d</td>\n",
       "      <td>discard-outlet</td>\n",
       "      <td>Legendary Creature — Human Spellshaper</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                        oracle_text  \\\n",
       "0  Abomination of Gudul  Flying\\nWhenever Abomination of Gudul deals co...   \n",
       "1         Academy Elite  Academy Elite enters the battlefield with X +1...   \n",
       "2        Academy Raider  Intimidate (This creature can't be blocked exc...   \n",
       "3     Akoum Flameseeker  Cohort — {T}, Tap an untapped Ally you control...   \n",
       "4    Alexi, Zephyr Mage  {X}{U}, {T}, Discard two cards: Return X targe...   \n",
       "\n",
       "                               oracleid             tag  \\\n",
       "0  3d98af5f-7a0b-4a5a-b3e4-f3c9d150c993  discard-outlet   \n",
       "1  ba6c3c72-c014-45c6-a0b4-59eb9a65303e  discard-outlet   \n",
       "2  75131d75-0703-44d0-b503-35190be8e66f  discard-outlet   \n",
       "3  efae637f-3232-46f2-9839-f3386e2f447d  discard-outlet   \n",
       "4  3f60de36-ed63-4d08-a012-fc16e91da46d  discard-outlet   \n",
       "\n",
       "                                type_line  label  \n",
       "0                       Creature — Horror      0  \n",
       "1                 Creature — Human Wizard      0  \n",
       "2                Creature — Human Warrior      0  \n",
       "3            Creature — Human Shaman Ally      0  \n",
       "4  Legendary Creature — Human Spellshaper      0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards_df = pd.read_csv(df_path)\n",
    "\n",
    "# Get numerical encoding of the card\n",
    "le = LabelEncoder()\n",
    "cards_df['label'] = le.fit_transform(cards_df['tag'])\n",
    "\n",
    "cards_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to normalize text (remove carriage return, tabs, punctuation...) and filter out English stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.replace('\\n', ' ').replace('\\t', '').replace('\\'', '')\n",
    "    text = re.split(r'\\W+', text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [word.translate(table) for word in text]\n",
    "    text = ' '.join([word.lower() for word in text if word != ''])\n",
    "    return text\n",
    "\n",
    "def filter_stop_words(text):\n",
    "    text = re.split(r'\\W+', text)\n",
    "    text = ' '.join([word.lower() for word in text if word not in STOP_WORDS])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply those functions on the cards *oracle_text*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df = cards_df.loc[:, ['oracle_text', 'label']].dropna()\n",
    "cards_df.loc[:,'normalized_oracle_text'] = cards_df['oracle_text'].apply(lambda x: filter_stop_words(normalize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oracle_text</th>\n",
       "      <th>label</th>\n",
       "      <th>normalized_oracle_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flying\\nWhenever Abomination of Gudul deals co...</td>\n",
       "      <td>0</td>\n",
       "      <td>flying abomination gudul deals combat damage p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Academy Elite enters the battlefield with X +1...</td>\n",
       "      <td>0</td>\n",
       "      <td>academy elite enters battlefield x 1 1 counter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intimidate (This creature can't be blocked exc...</td>\n",
       "      <td>0</td>\n",
       "      <td>intimidate creature cant blocked artifact crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cohort — {T}, Tap an untapped Ally you control...</td>\n",
       "      <td>0</td>\n",
       "      <td>cohort t tap untapped ally control discard car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{X}{U}, {T}, Discard two cards: Return X targe...</td>\n",
       "      <td>0</td>\n",
       "      <td>x u t discard cards return x target creatures ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         oracle_text  label  \\\n",
       "0  Flying\\nWhenever Abomination of Gudul deals co...      0   \n",
       "1  Academy Elite enters the battlefield with X +1...      0   \n",
       "2  Intimidate (This creature can't be blocked exc...      0   \n",
       "3  Cohort — {T}, Tap an untapped Ally you control...      0   \n",
       "4  {X}{U}, {T}, Discard two cards: Return X targe...      0   \n",
       "\n",
       "                              normalized_oracle_text  \n",
       "0  flying abomination gudul deals combat damage p...  \n",
       "1  academy elite enters battlefield x 1 1 counter...  \n",
       "2  intimidate creature cant blocked artifact crea...  \n",
       "3  cohort t tap untapped ally control discard car...  \n",
       "4  x u t discard cards return x target creatures ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create *word2vec* Embedding for *oracle_text*\n",
    "We create a 100 dimensional word embedding for all the cards *oracle_text*\n",
    "\n",
    "**NB: those word embeddings can be created just once, skip this section if already done**\n",
    "\n",
    "First we fetch the *oracle_text* for all the available cards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(database=\"mtg\", user=\"postgres\", password=\"postgres\", port=5432, host='localhost')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"select oracle_text from cards where exists (select 1 from jsonb_each_text(cards.legalities) j where j.value not like '%not_legal%') and lang='en';\")\n",
    "\n",
    "cards = []\n",
    "card = cur.fetchone()\n",
    " \n",
    "while card is not None:\n",
    "    card = cur.fetchone()\n",
    "    cards.append(card)\n",
    " \n",
    "cur.close()\n",
    "\n",
    "cards = cards[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we normalize all those *oracle_text*s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = [filter_stop_words(normalize(card[0])) for card in cards if card[0]]\n",
    "cards = [card.split(' ') for card in cards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many distinct words are there in the cards? we will need it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8949 distinct words in the cards\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "card_words = list(set(list(it.chain.from_iterable(cards))))\n",
    "print(f'There are {len(card_words)} distinct words in the cards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third we create the *Word2Vec* representations for all the words in the normalized *oracle_text*s and persist them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import gzip\n",
    "\n",
    "\n",
    "path = get_tmpfile(\"./data/word2vec.model\")\n",
    "\n",
    "model = Word2Vec(cards, size=word_dim, window=5, min_count=1, workers=4)\n",
    "model.wv.save_word2vec_format(\"../../data/word2vec.txt\")\n",
    "\n",
    "# gzip the model\n",
    "f_in = open('../../data/word2vec.txt', 'rb')\n",
    "f_out = gzip.open('../../data/word2vec.txt.gz', 'wb')\n",
    "f_out.writelines(f_in)\n",
    "f_out.close()\n",
    "f_in.close()\n",
    "\n",
    "# Then command line:\n",
    "# python3.6 -m spacy init-model en ./data/spacy.word2vec.model --vectors-loc data/word2vec.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last step is to create a SpaCy model based on those persisted *Word2Vec* representations by executing:\n",
    "\n",
    "`python3.6 -m spacy init-model en ./data/spacy.word2vec.model --vectors-loc data/word2vec.txt.gz`\n",
    "\n",
    "in the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Card text as a vector\n",
    "\n",
    "The *SpaCy* model aggregates the (100-dimensional) vectors of the words in the cards (resp. in their *oracle_text*) into a single (100-dimensional) vector. For each card, we create this vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: UserWarning: [W019] Changing vectors name from en_model.vectors to en_model.vectors_8949, to avoid clash with previously loaded vectors. See Issue #3853.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "from spacy import load\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "nlp_mtg = load('../../data/spacy.word2vec.model')\n",
    "\n",
    "X = zeros((cards_df.shape[0], word_dim, 1))\n",
    "for i, text in enumerate(cards_df['normalized_oracle_text']):\n",
    "    X[i,:] = nlp_mtg(text).vector.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will, for each card it will predict, a 6 dimensional array. The value in each dimension will correspond to the probability for the card to belong to the corresponding tag.\n",
    "\n",
    "For this, we have to provide `y` as a 6 dimensional vector. We *one-hot-encode* the labels: \n",
    "* *0* => [1, 0, 0, 0, 0, 0]\n",
    "* *1* => [0, 1, 0, 0, 0, 0]\n",
    "* etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "y = cards_df['label'].values.reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "y =  enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3334, 6)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test, with 90% allocated to train and 10% to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<334x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 334 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Bidirectional, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model consisting in various dense layers with some pooling in the middle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = y.shape[1]\n",
    "batch_size = 32\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(word_dim, 1, ))\n",
    "x = Dense(60, activation='relu')(sequence_input)\n",
    "x = Dense(30, activation='relu')(x)\n",
    "# x = Conv1D(15, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool])\n",
    "x = Dense(60, activation='relu')(x)\n",
    "preds = Dense(n_labels, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "2624/2700 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8320\n",
      "Epoch 00001: val_acc improved from -inf to 0.83333, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 2s 664us/sample - loss: 0.4357 - acc: 0.8320 - val_loss: 0.3515 - val_acc: 0.8333\n",
      "Epoch 2/50\n",
      "2624/2700 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8418\n",
      "Epoch 00002: val_acc did not improve from 0.83333\n",
      "2700/2700 [==============================] - 0s 152us/sample - loss: 0.3664 - acc: 0.8419 - val_loss: 0.3475 - val_acc: 0.8328\n",
      "Epoch 3/50\n",
      "2688/2700 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8441\n",
      "Epoch 00003: val_acc improved from 0.83333 to 0.85056, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 0s 159us/sample - loss: 0.3624 - acc: 0.8442 - val_loss: 0.3422 - val_acc: 0.8506\n",
      "Epoch 4/50\n",
      "2624/2700 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8485\n",
      "Epoch 00004: val_acc did not improve from 0.85056\n",
      "2700/2700 [==============================] - 0s 155us/sample - loss: 0.3591 - acc: 0.8481 - val_loss: 0.3380 - val_acc: 0.8439\n",
      "Epoch 5/50\n",
      "2656/2700 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8504\n",
      "Epoch 00005: val_acc improved from 0.85056 to 0.85278, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 0s 163us/sample - loss: 0.3550 - acc: 0.8501 - val_loss: 0.3318 - val_acc: 0.8528\n",
      "Epoch 6/50\n",
      "2336/2700 [========================>.....] - ETA: 0s - loss: 0.3559 - acc: 0.8472\n",
      "Epoch 00006: val_acc improved from 0.85278 to 0.86611, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 0s 157us/sample - loss: 0.3528 - acc: 0.8487 - val_loss: 0.3288 - val_acc: 0.8661\n",
      "Epoch 7/50\n",
      "2688/2700 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8525\n",
      "Epoch 00007: val_acc improved from 0.86611 to 0.86722, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 0s 160us/sample - loss: 0.3499 - acc: 0.8525 - val_loss: 0.3283 - val_acc: 0.8672\n",
      "Epoch 8/50\n",
      "2496/2700 [==========================>...] - ETA: 0s - loss: 0.3488 - acc: 0.8522\n",
      "Epoch 00008: val_acc improved from 0.86722 to 0.86778, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 0s 169us/sample - loss: 0.3488 - acc: 0.8531 - val_loss: 0.3244 - val_acc: 0.8678\n",
      "Epoch 9/50\n",
      "2624/2700 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8514\n",
      "Epoch 00009: val_acc improved from 0.86778 to 0.86778, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 0s 168us/sample - loss: 0.3484 - acc: 0.8514 - val_loss: 0.3227 - val_acc: 0.8678\n",
      "Epoch 10/50\n",
      "2688/2700 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8550\n",
      "Epoch 00010: val_acc improved from 0.86778 to 0.87000, saving model to ../../data/weights_dl_model.hdf5\n",
      "2700/2700 [==============================] - 1s 202us/sample - loss: 0.3469 - acc: 0.8548 - val_loss: 0.3236 - val_acc: 0.8700\n",
      "Epoch 11/50\n",
      "2560/2700 [===========================>..] - ETA: 0s - loss: 0.3459 - acc: 0.8535\n",
      "Epoch 00011: val_acc did not improve from 0.87000\n",
      "2700/2700 [==============================] - 0s 156us/sample - loss: 0.3464 - acc: 0.8530 - val_loss: 0.3260 - val_acc: 0.8511\n",
      "Epoch 12/50\n",
      "2496/2700 [==========================>...] - ETA: 0s - loss: 0.3446 - acc: 0.8553\n",
      "Epoch 00012: val_acc did not improve from 0.87000\n",
      "2700/2700 [==============================] - 0s 173us/sample - loss: 0.3450 - acc: 0.8544 - val_loss: 0.3268 - val_acc: 0.8572\n",
      "Epoch 13/50\n",
      "2624/2700 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8536\n",
      "Epoch 00013: val_acc did not improve from 0.87000\n",
      "2700/2700 [==============================] - 0s 173us/sample - loss: 0.3455 - acc: 0.8538 - val_loss: 0.3216 - val_acc: 0.8656\n",
      "Epoch 14/50\n",
      "2560/2700 [===========================>..] - ETA: 0s - loss: 0.3431 - acc: 0.8583\n",
      "Epoch 00014: val_acc did not improve from 0.87000\n",
      "2700/2700 [==============================] - 0s 161us/sample - loss: 0.3439 - acc: 0.8571 - val_loss: 0.3236 - val_acc: 0.8550\n",
      "Epoch 15/50\n",
      "2560/2700 [===========================>..] - ETA: 0s - loss: 0.3429 - acc: 0.8576\n",
      "Epoch 00015: val_acc did not improve from 0.87000\n",
      "2700/2700 [==============================] - 1s 210us/sample - loss: 0.3433 - acc: 0.8575 - val_loss: 0.3180 - val_acc: 0.8583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff7a50925f8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filepath = '../../data/weights_dl_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor='val_acc', mode='max', patience=5)\n",
    "callbacks = [checkpoint, early]\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=50, validation_split=.1, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reach roughly 86% accuracy like that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card Text as word embeddings\n",
    "Here each word (that has an embedding) is concatenated and we get a matrix for each text (padded with 0 if too long):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(card_words)\n",
    "\n",
    "embeddings_index = np.zeros((n_words + 1, 100))\n",
    "for idx, word in enumerate(card_words):\n",
    "    try:\n",
    "        embedding = nlp_mtg.vocab[word].vector\n",
    "        embeddings_index[idx] = embedding\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximal length of a card is 58\n"
     ]
    }
   ],
   "source": [
    "max_length_card = max([len(card) for card in cards])\n",
    "print(f'The maximal length of a card is {max_length_card}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the implementation of a quite clean and generic deep learning text classification class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardClassifier(BaseEstimator, TransformerMixin):\n",
    "    '''Wrapper class for keras text classification models that takes raw text as input.'''\n",
    "  \n",
    "    def __init__(self, max_words=10000, input_length=30, emb_dim=100, n_classes=6, epochs=100, batch_size=32, emb_idx=0, lr=1e-3, model_path='/tmp/text_classification.hdf5'):\n",
    "        self.max_words = max_words\n",
    "        self.input_length = input_length\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.epochs = epochs\n",
    "        self.bs = batch_size\n",
    "        self.embeddings_index = emb_idx\n",
    "        self.lr = lr\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words+1, lower=True, split=' ')\n",
    "        self.model = self._get_model()\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def _get_model(self):\n",
    "        input_text = Input((self.input_length,))\n",
    "        text_embedding = Embedding(input_dim=self.max_words+1, output_dim=self.emb_dim, input_length=self.input_length, \n",
    "                                   mask_zero=False, weights=[self.embeddings_index], trainable=False)(input_text)\n",
    "        text_embedding = SpatialDropout1D(0.4)(text_embedding)\n",
    "        bilstm =(LSTM(units=50, recurrent_dropout=0.2, return_sequences = True))(text_embedding)\n",
    "        x = Dropout(0.2)(bilstm)\n",
    "        x =(LSTM(units=50,  recurrent_dropout=0.2, return_sequences = True))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x =(LSTM(units=50,  recurrent_dropout=0.2))(x)\n",
    "        out = Dense(units=self.n_classes, activation=\"softmax\")(x)\n",
    "        model = Model(inputs=[input_text],outputs=[out])\n",
    "        \n",
    "        model.compile(optimizer=Adam(lr=self.lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "  \n",
    "    def _get_sequences(self, texts):\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(seqs, maxlen=self.input_length, value=0)\n",
    "  \n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}\n",
    "        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1\n",
    "        seqs = self._get_sequences(X)\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor='val_acc', mode='max', patience=5)\n",
    "        callbacks = [checkpoint, early]\n",
    "        self.model.fit([seqs ], y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1, callbacks=callbacks)\n",
    "  \n",
    "    def predict_proba(self, X, y=None):\n",
    "        seqs = self._get_sequences(X)\n",
    "        return self.model.predict(seqs)\n",
    "  \n",
    "    def predict(self, X, y=None):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "  \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(np.argmax(y, axis=1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_16 (Embedding)     (None, 20, 100)           895000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_14 (Spatia (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 20, 50)            30200     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 20, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 965,906\n",
      "Trainable params: 70,906\n",
      "Non-trainable params: 895,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model_path = '../../data/dl_text_classification.hdf5'\n",
    "card_model = CardClassifier(emb_idx=embeddings_index, max_words=n_words, input_length=20, batch_size=batch_size, lr=1e-2, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/100\n",
      "2700/2700 [==============================] - 21s 8ms/step - loss: 1.1103 - acc: 0.5881 - val_loss: 0.9768 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64333, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 2/100\n",
      "2700/2700 [==============================] - 13s 5ms/step - loss: 1.0144 - acc: 0.6178 - val_loss: 0.9669 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64333\n",
      "Epoch 3/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.9695 - acc: 0.6296 - val_loss: 0.9105 - val_acc: 0.6633\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64333 to 0.66333, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 4/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.8857 - acc: 0.6774 - val_loss: 0.7944 - val_acc: 0.7033\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.66333 to 0.70333, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 5/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.8325 - acc: 0.7052 - val_loss: 0.7330 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70333 to 0.76333, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 6/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.8100 - acc: 0.7304 - val_loss: 0.6948 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76333 to 0.79000, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 7/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.7559 - acc: 0.7552 - val_loss: 0.7321 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79000\n",
      "Epoch 8/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.7315 - acc: 0.7570 - val_loss: 0.6503 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79000\n",
      "Epoch 9/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6930 - acc: 0.7781 - val_loss: 0.6551 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.79000 to 0.80333, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 10/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.7008 - acc: 0.7748 - val_loss: 0.6464 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80333\n",
      "Epoch 11/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6754 - acc: 0.7811 - val_loss: 0.6736 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80333\n",
      "Epoch 12/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6599 - acc: 0.7915 - val_loss: 0.6187 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80333\n",
      "Epoch 13/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6453 - acc: 0.7889 - val_loss: 0.6994 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80333\n",
      "Epoch 14/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6553 - acc: 0.7893 - val_loss: 0.6499 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.80333 to 0.82000, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 15/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6152 - acc: 0.8067 - val_loss: 0.6462 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82000\n",
      "Epoch 16/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6104 - acc: 0.8115 - val_loss: 0.6116 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.82000\n",
      "Epoch 17/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5934 - acc: 0.8044 - val_loss: 0.6437 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.82000 to 0.82333, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 18/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.6172 - acc: 0.8015 - val_loss: 0.6251 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.82333\n",
      "Epoch 19/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5850 - acc: 0.8181 - val_loss: 0.6149 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.82333 to 0.82667, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 20/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5750 - acc: 0.8130 - val_loss: 0.6542 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82667\n",
      "Epoch 21/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5643 - acc: 0.8222 - val_loss: 0.6176 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.82667\n",
      "Epoch 22/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5647 - acc: 0.8248 - val_loss: 0.6050 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82667\n",
      "Epoch 23/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5584 - acc: 0.8133 - val_loss: 0.6074 - val_acc: 0.8367\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.82667 to 0.83667, saving model to ../../data/dl_text_classification.hdf5\n",
      "Epoch 24/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5497 - acc: 0.8289 - val_loss: 0.6005 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83667\n",
      "Epoch 25/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5424 - acc: 0.8230 - val_loss: 0.5729 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83667\n",
      "Epoch 26/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5356 - acc: 0.8259 - val_loss: 0.6140 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83667\n",
      "Epoch 27/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5457 - acc: 0.8215 - val_loss: 0.5841 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83667\n",
      "Epoch 28/100\n",
      "2700/2700 [==============================] - 14s 5ms/step - loss: 0.5283 - acc: 0.8293 - val_loss: 0.6078 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83667\n"
     ]
    }
   ],
   "source": [
    "X, y = cards_df['normalized_oracle_text'], cards_df['label'].values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=7, stratify=y)\n",
    "y_train, y_test =  enc.fit_transform(y_train), enc.fit_transform(y_test)\n",
    "card_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083832335329342"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this model we reach 81% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
